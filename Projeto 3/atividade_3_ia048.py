# -*- coding: utf-8 -*-
"""Atividade_3_IA048.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s2hC8igj0dPT4GTB5fFI0y9CxoD53Gh4
"""

pip install medmnist

from medmnist import BloodMNIST
dataset_Teste = BloodMNIST(split="test", download=True, size=28)

dataset_Train = BloodMNIST(split="train", download=True, size=28)

dataset_Validation = BloodMNIST(split="val", download=True, size=28)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from medmnist import BloodMNIST
from medmnist import INFO
from torchvision import transforms

# Definir transformações
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])])

# Carregar dados BloodMNIST
train_dataset = BloodMNIST(split='train', transform=transform, download=True)
test_dataset = BloodMNIST(split='test', transform=transform, download=True)

# Extrair dados e rótulos
x_train, y_train = train_dataset.imgs, train_dataset.labels
x_test, y_test = test_dataset.imgs, test_dataset.labels

# Normalizar os dados
x_train = x_train / 255.0
x_test = x_test / 255.0

# Convertendo os rótulos para one-hot encoding
y_train = to_categorical(y_train, num_classes=8)
y_test = to_categorical(y_test, num_classes=8)

model = Sequential()
model.add(Flatten(input_shape=(28, 28, 3)))
model.add(Dense(128, activation='relu'))
model.add(Dense(8, activation='softmax'))

# Compilar o modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test))

# Avaliação no conjunto de teste
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Accuracy: {accuracy}')

# Prever as classes do conjunto de teste
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Matriz de confusão
conf_matrix = confusion_matrix(y_true, y_pred_classes)

# Plotando a matriz de confusão
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(8), yticklabels=range(8))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import mnist

# (x_train, y_train), (x_test, y_test) = mnist.load_data()
# (x_train, y_train), (x_test, y_test) = (dataset_Train), (dataset_Teste)
x_train, y_train = train_dataset.imgs, train_dataset.labels
x_test, y_test = test_dataset.imgs, test_dataset.labels
# x_train, y_train = dataset_Train.imgs, dataset_Train.labels
# x_teste, y_test = dataset_Teste.imgs, dataset_Teste.labels

x_train = x_train[..., np.newaxis] / 255.0
x_test = x_test[..., np.newaxis] / 255.0
y_train = to_categorical(y_train, 8)
y_test = to_categorical(y_test, 8)

def create_model(num_kernels, kernel_size):
    model = Sequential([
        Conv2D(num_kernels, kernel_size=kernel_size, activation='relu', input_shape=(28, 28, 3)),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(8, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Parâmetros para testar
kernel_numbers = [8, 16, 32]
kernel_sizes = [(3, 3), (5, 5), (7, 7)]

# Função para treinar e avaliar o modelo
def train_and_evaluate(num_kernels, kernel_size):
  model = create_model(num_kernels, kernel_size)
  model.fit(x_train, y_train, epochs=40, batch_size=32, validation_data=(x_test, y_test))
  loss, accuracy = model.evaluate(x_test, y_test)
  return accuracy

# Testar diferentes combinações de parâmetros
results = {}
for num_kernels in kernel_numbers:
    for kernel_size in kernel_sizes:
        accuracy = train_and_evaluate(num_kernels, kernel_size)
        results[(num_kernels, kernel_size)] = accuracy
        print(f'Num Kernels: {num_kernels}, Kernel Size: {kernel_size}, Accuracy: {accuracy}')

# Exibir os resultados
for params, accuracy in results.items():
    print(f'Kernels: {params[0]}, Kernel Size: {params[1]} => Accuracy: {accuracy}')

best_params = max(results, key=results.get)
best_model = create_model(best_params[0], best_params[1])
best_model.fit(x_train, y_train, epochs=40, validation_data=(x_test, y_test))

# Avaliar o modelo final
loss, accuracy = best_model.evaluate(x_test, y_test)
print(f'Melhor configuração - Num Kernels: {best_params[0]}, Kernel Size: {best_params[1]}, Accuracy: {accuracy}')

from sklearn.metrics import classification_report

# Predições
y_pred = best_model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Matriz de confusão
conf_matrix = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Relatório de classificação
print(classification_report(y_true, y_pred_classes))


# Identificar exemplos de erros
errors = (y_pred_classes != y_true)
error_indices = np.where(errors)[0]
for i, idx in enumerate(error_indices[:5]):
    print(f"Example {i+1}:")
    print(f"True label: {y_true[idx]}, Predicted label: {y_pred_classes[idx]}")
    plt.imshow(x_test[idx].reshape(28,28,3), cmap='gray')
    plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Add, Activation
from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
import numpy as np
from tensorflow.keras.datasets import mnist

# (x_train, y_train), (x_test, y_test) = mnist.load_data()
# (x_train, y_train), (x_test, y_test) = (dataset_Train), (dataset_Teste)
x_train, y_train = train_dataset.imgs, train_dataset.labels
x_test, y_test = test_dataset.imgs, test_dataset.labels
# x_train, y_train = dataset_Train.imgs, dataset_Train.labels
# x_teste, y_test = dataset_Teste.imgs, dataset_Teste.labels

x_train = x_train[..., np.newaxis] / 255.0
x_test = x_test[..., np.newaxis] / 255.0
y_train = to_categorical(y_train, 8)
y_test = to_categorical(y_test, 8)

def resnet_block(inputs, filters, kernel_size, strides, use_activation=True):
    x = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)
    x = BatchNormalization()(x)
    if use_activation:
        x = Activation('relu')(x)
    return x

def residual_block(inputs, filters, kernel_size, strides):
    x = resnet_block(inputs, filters, kernel_size, strides)
    x = resnet_block(x, filters, kernel_size, 1, use_activation=False)
    shortcut = Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(inputs)
    shortcut = BatchNormalization()(shortcut)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

def create_resnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = resnet_block(inputs, 64, (7, 7), 2)
    x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)

    x = residual_block(x, 64, (3, 3), 1)
    x = residual_block(x, 64, (3, 3), 1)

    x = residual_block(x, 128, (3, 3), 2)
    x = residual_block(x, 128, (3, 3), 1)

    x = residual_block(x, 256, (3, 3), 2)
    x = residual_block(x, 256, (3, 3), 1)

    x = residual_block(x, 512, (3, 3), 2)
    x = residual_block(x, 512, (3, 3), 1)

    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Criar a ResNet
input_shape = (28, 28, 3)
num_classes = 8
resnet_model = create_resnet(input_shape, num_classes)

# Treinar o modelo ResNet
resnet_model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))

# Avaliar o modelo ResNet
loss, accuracy = resnet_model.evaluate(x_test, y_test)
print(f'Acurácia global da ResNet: {accuracy}')